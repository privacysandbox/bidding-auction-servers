# Cost Estimation Tool

For highlevel overview of the cost estimation tool refer to the
[Cost estimation tool explainer](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_cost_estimation_tool.md).

The cost estimation tool helps estimate usage and cost for deployed and running Bidding and Auction
(B&A) services. It does so by downloading metrics collected by the system and applying a set of
formulas to it. This set of formulas is called a cost model.

The cost model takes as input the set of metrics generated by the system and can produce estimates
per [SKU](https://en.wikipedia.org/wiki/Stock_keeping_unit), the billing entity in a cloud platform.

The tool takes as input a file containing one or more cost models. It works with one specified cost
model at a time. It downloads usage and performance metrics specified in the cost model from a B&A
deployment and applies them to the cost model's formulas to produce a per-SKU usage estimate.

Once the usage is estimated, the tool can also optionally take a SKU file. This file contains
pricing information for each of the SKUs that were used. If this file is provided, the tool will
multiply the per-SKU usage estimates with the given prices, and produce per-SKU cost estimates.

The tool outputs this information as a CSV file that can be opened with a spreadsheet.

## Setup (Linux)

Before running the tool, make sure you have the following setup (commands given for a Debian/Linux
system):

### Docker

The cost estimation tool is a part of the Bidding and auction repository, so follow pre-requisites
[here](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_aws_guide.md#step-0-prerequisites)
for downloading and initializing the repository.

This code relies on [Docker](https://docs.docker.com/) for building and execution. Please
[install Docker](https://docs.docker.com/get-started/get-docker/) on your system before using the
tool.

## Usage

### Run the tool using Docker

Sample command for running an AWS seller model:

```sh
  cd $(git rev-parse --show-toplevel)
  ./run_in_docker \
  --cost_model aws_buyer_us_west_1 \
  --aws_metrics_download myenv "2024-12-12T00:00:00-07:00" "2024-12-15T00:00:00-07:00" \
  --param bfe:num_instances 1 \
  --param bidding:num_instances 1
```

Sample command for running an GCP buyer model:

```sh
  cd $(git rev-parse --show-toplevel)
  ./run_in_docker \
  --cost_model gcp_seller_us_east_4 \
  --gcp_metrics_download myproject myenv "2024-12-12T00:00:00-07:00" "2024-12-15T00:00:00-07:00" \
  --param sfe:num_instances 1 \
  --param auction:num_instances 1
```

> **_NOTE:_** The above commands use the default values for `cost_model_file` and `sku_file`

The above commands use a wrapper script `run_in_docker` to build and run the tool in docker. This
builds a docker image with all the required depdendencies and runs the tool.

#### File mapping

The wrapper script takes care of mapping the input and output files so that the input files are
available to the docker image, and the output files are availabe on the host once the tool finishes
running. Absolute pathnames are mapped to the same path inside docker. Relative pathnames are mapped
to the same path under the `/app/` folder. Due to this reason, relative pathnames that land outside
the current working directory will not work. For example, `../../` will not work, but `a/b/c` should
work fine.

> **_NOTE:_** The wrapper script parses the command line args and adds input and output file
> mappings for docker. For this to work, the files need to be separated from the option by spaces.
> E.g. `--metrics_file /tmp/aws.csv` should work fine, but `--metrics_file=/tmp/aws.csv` will not.

#### Default files

The cost.yaml and sku.json file from the tool's source directory are copied as an artifact into the
docker image. This means you can edit the cost.yaml or sku.json files in the source directory and
have them be automatically picked up by the tool without specifying either of the file options.

We write out the full paths of any file written or read from at the INFO log level. Add
`--loglevel info` to your command to have the tool print the full path of each file that is read
from or written to.

> **_NOTE:_** The output file names printed are the names of the files in the docker image. When
> default files are used by omitting them in the options, the printed names will be under the `/app`
> directory, where the source is copied.

### Authentication with cloud providers

The tools downloads metrics from supported cloud providers. This requires authentication. See the
sections [--aws_metrics_download](#--aws_metrics_download) and
[--gcp_metrics_download](#--gcp_metrics_download) for details of setting up this auth. The wrapper
script described above will propagate this auth to the docker image.

For AWS, this is done by propagating the environment variables: `AWS_ACCESS_KEY_ID` and
`AWS_SECRET_ACCESS_KEY`. Please make sure these environment variables are set if you're using AWS.

For GCP, this is done by making the credentials file
`$HOME/.config/gcloud/application_default_credentials.json` accessible to the docker image by
mounting it with [--volume](https://docs.docker.com/engine/storage/volumes/). Please make sure this
file is available if you're using GCP.

### Tool Options

```sh
usage: main.py  --cost_model COST_MODEL_NAME
                [--help]
                [--loglevel {info,debug,warning,error,fatal}]
                [--cost_model_file COST_MODEL_FILENAME]
                [--sku_file SKU_FILENAME]
                [--output_file OUTPUT_FILENAME]
                [--metrics_file METRICS_FILENAME]
                [--aws_metrics_download ENVIRONMENT START_TIME END_TIME]
                [--gcp_metrics_download PROJECT ENVIRONMENT START_TIME END_TIME]
                [--save_downloaded_metrics METRICS_FILENAME]
                [--param ['KEY', 'VALUE'] ['KEY', 'VALUE'] ... ]

options:
  -h, --help
        show this help message and exit

  --loglevel {info,debug,warning,error,fatal}
        Provide logging level. Example --loglevel debug, default=warning

  --cost_model_file COST_MODEL_FILENAME
        Provide the cost model yaml file. Default: tools/cost_estimation/cost.yaml

  --cost_model COST_MODEL_NAME
        Select the cost model to use

  --sku_file SKU_FILENAME
        JSON-formatted file providing SKU pricing. Default: tools/cost_estimation/sku.json

  --output_file OUTPUT_FILENAME
        Saves output to a file instead of the default, which prints to STDOUT

  --metrics_file METRICS_FILENAME
        Csv-formatted file providing metrics for use in the cost model

  --aws_metrics_download ENVIRONMENT START_TIME END_TIME
        Download metrics from AWS. Args are <environment> <start time> <end time>. Only works if the selected cost
        model is an AWS model.

  --gcp_metrics_download PROJECT ENVIRONMENT START_TIME END_TIME
        Download metrics from GCP. Args are <project> <environment> <start time> <end time>. Only works if the selected
        cost model is a GCP model.

  --save_downloaded_metrics METRICS_FILENAME
        Save the downloaded metrics in a Csv file. This file can be used with the --metrics_file option in the future.

  --param ['KEY', 'VALUE'] ['KEY', 'VALUE']
        Arbitrary key-value parameters to pass as variables to the cost model. This argument can be given multiple
        times to pass multiple key-values into the cost model.
```

#### --cost_model

This specifies the cost model to use. Cost models are loaded from the cost_model_file. If the
cost_model is not found in the given file, a message shows the available cost models which you can
choose from.

#### --help

Use this option to print a detailed help message with all options listed.

#### --loglevel {info,debug,warning,error,fatal}

Use this option to set the logging verbosity of the tool.

#### --cost_model_file filename

This specifies the YAML file to load cost models from. Detailed description of the structure can be
found in the [Cost Model](#cost-model) section.

#### --sku_file filename

This specifies the JSON file to load the SKU pricing data from. This file contains pricing
information which is used to calculate the cost estimates. If this file is not given, only usage
estimates are calculated and all cost estimates will be 0.

The file can contain AWS or GCP SKUs, or both. The tool will use the appropriate SKUs based on the
cost model selected. The file should be well-formatted JSON. The tool will fail if the file is not
well-formatted. The SKUs in the file should contain the following fields:

-   `region`: The region that the SKU is for. A cost model will use only the SKUs that match its
    region.
-   `description`: The description of the SKU. This should match strings used in the usage
    estimations for the cost model.
-   `sku_id`: The SKU ID.
-   `unit_cost`: The unit cost of the SKU (per cost_basis).
-   `cost_basis`: The cost basis of the SKU (e.g. "HR", "GB").

JSON structure of the file should be as follows:

```json
{
  "aws": [
    {
      "region": "us-east-1",
      "description": "BoxUsage:m5.4xlarge",
      "sku_id": "",
      "unit_cost": INSERT_NEGOTIATED_SKU_PRICE,
      "cost_basis": "HR"
    },
    {
      "region": "us-west-1",
      "description": "LCUUsage",
      "sku_id": "",
      "unit_cost": INSERT_NEGOTIATED_SKU_PRICE,
      "cost_basis": "GB"
    },
    ...
  ],
  "gcp": [
    {
      "region": "us-east4",
      "description": "N2D AMD Instance Core running in Virginia",
      "sku_id": "809C-1E3B-306E",
      "unit_cost": INSERT_NEGOTIATED_SKU_PRICE,
      "cost_basis": "HR"
    },
    {
      "region": "us-east1",
      "description": "Network Inter Region Data Transfer Out from Americas to Virginia",
      "sku_id": "C863-37DA-506E",
      "unit_cost": INSERT_NEGOTIATED_SKU_PRICE,
      "cost_basis": "GB"
    },
    ...
  ]
}
```

#### --output_file filename

Saves output to a file with the given name instead of printing to STDOUT (the default).

#### --metrics_file filename

This specifies the CSV file to load the metrics data from. The metrics file is expected to be a well
formatted CSV file with each line of the form:

```csv
<metric>, <value>
```

#### --aws_metrics_download

**You need
[AWS credentials](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html) set
up for the tool to have access to downloading metrics.**

This option takes the parameters \<environment> \<start time> \<end time>.

-   Environment is the
    [AWS deployment environment](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_aws_guide.md#terraform-environment-directory)
    that you're estimating cost for.
-   Start and end times are for the start and end of your test, given in
    [iso format](https://docs.python.org/3/library/datetime.html#datetime.date.fromisoformat).
    Results are calculated using metrics data from this time period.

This option downloads the metrics required for the model from AWS. This option only works if the
selected cost model (through the --cost-model option) is an AWS model. The region and the metrics
required are read directly from the cost model

#### --gcp_metrics_download

**You need [GCP credentials](https://cloud.google.com/docs/authentication/gcloud) available to have
access to download metrics. See
[gcloud installation and initialization instructions](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_gcp_guide.md#step-0-prerequisites)
for getting set up.**

This option takes the parameters \<project> \<environment> \<start time> \<end time>.

-   Project is the
    [GCP project](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_gcp_guide.md#gcp-project-setup)
    for your deployment
-   Environment is the
    [GCP deployment environment](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_gcp_guide.md#step-12-building-the-gcp-confidential-space-docker-image)
    that you're estimating cost for.
-   Start and end times are for the start and end of your test, given in
    [iso format](https://docs.python.org/3/library/datetime.html#datetime.date.fromisoformat).
    Results are

It downloads the metrics required for the model from GCP. This option only works if the selected
cost model (through the --cost-model option) is a GCP model. The metrics required are read directly
from the cost model

#### --save_downloaded_metrics filename

Saves downloaded metrics to the given file. This option only works when downloading metrics. If
metrics were loaded from a file (with the --metrics_file option), metrics will not be saved.

#### --param ['KEY', 'VALUE'] ...

This option provides the ability to pass any number of arbitrary key-value pairs to the cost model.
The key becomes the variable name, which will be accessible to the cost model. This can be used to
provide deployment specific data like number of servers used etc. to the cost model. E.g. you can
pass the sfe:num_instances, bfe:num_instance etc. to pass the number of instances for each service
your cost model needs.

## Cost model

The cost model file is a [yaml](https://en.wikipedia.org/wiki/YAML) file that can contain multiple
cost models. This file should be well formatted yaml with document separators used between different
cost models. A cost model should start with three dashes `---` and end with three dots `...` like
so:

```YAML
---
cost-model-1 yaml
...
---
cost-model-2 yaml
...
```

### Structure of the cost model

The tool processes sections of the yaml in the following order:

-   `cost_model_metadata`
-   [Optional] `download_metrics`
-   [Optional] `defined_values`
-   `usage_estimations`
-   [Optional] `results_template`

Some of these sections define variables for later use. This allows for the creation of cost model
formulas that can reference previously defined metrics or values in the formula.

#### Cost model metadata section

A `cost_model_metadata` field with the following structure:

```YAML
cost_model_metadata:
  name: <name of the model>
  description: <optional description>
  vendor: <vendor e.g. gcp/aws>
  region: <region for the model, e.g. us-west-1>
  num_requests_metric: <metric used to compute per-million metrics for the model>
```

#### Download metrics section

This is an optional section that lists the metrics to download. These can also be provided from a
csv file. These are metrics exposed as variables for the rest of the cost model to use.

The complete list of metrics recorded by B&A is available in the
[Monitoring explainer](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/monitoring_protected_audience_api_services.md#list-of-metrics)

```YAML
download_metrics:
  - metric: request.size_bytes
    service: ["sfe", "auction", "bfe", "bidding"]
```

The above yaml will download the metric `request.size_bytes` for each of the services sfe, auction,
bfe and bidding. They will be exposed as the following variables to the cost model:

-   `sfe:request.size_bytes`
-   `bfe:request.size_bytes`
-   `auction:request.size_bytes`
-   `bidding:request.size_bytes`

You can also optionally specify a label. Labels may contain spaces, which will be converted to `_`
when exposed as a variable. Consider the following metric:

```yaml
- service: ['sfe', 'auction']
  metric: system.cpu.percent
  label: total cpu cores
```

This will be expanded to fetch the metrics from the two services and be exposed as the following
variable to the cost model:

-   `sfe:system.cpu.percent:total_cpu_cores`
-   `auction:system.cpu.percent:total_cpu_cores`

Since the variable names can become pretty long, you can optionally specify a new name for the
variable using the copy_variable_to option. For example,

```yaml
- service: ['sfe', 'auction']
  metric: system.cpu.percent
  label: total cpu cores
  copy_variable_to: total_cores
```

The above will result in the following variables being available to the cost model:

-   `sfe:system.cpu.percent:total_cpu_cores`
-   `auction:system.cpu.percent:total_cpu_cores`
-   `sfe:total_cores`
-   `auction:total_cores`

The original variable names remain available, but they are also accesible through a new name that
was mentioned in the copy_variables_to option.

The value of each of the metrics that gets downloaded is defined for future formulas to use. These
can be used in the `defined_values` or the `usage_estimations` sections.

##### Metric download and aggregation

The metrics are downloaded from the respective cloud platforms and are aggregated. For each metric,
a single number is produced at the end of the download and aggregation step.

For GCP, we use the
[list_time_series](https://cloud.google.com/python/docs/reference/monitoring/latest/google.cloud.monitoring_v3.types.ListTimeSeriesRequest)
API to download metric data. The downloaded data is then aggregated and a single number is produced
for each metric. The aggregation mechanism does not need to be defined in the YAML and is available
as part of the data returned by the API.

For AWS, we use the boto3
[get_metric_statistics](https://boto3.amazonaws.com/v1/documentation/api/1.35.9/reference/services/cloudwatch/client/get_metric_statistics.html)
API to download metric data. For this to work, the `aggregation` attribute for each metric must be
specified in the YAML. This attribute can take the values `Average` and `Sum` (case sensitive).

For example (**AWS only**):

```yaml
- metric: response.size_bytes
  aggregation: Average
  service: [bfe, bidding]
```

#### Defined values

This section helps define the compute intermediate values that occur repeatedly in your formula. For
example, some metrics (like `request.size_bytes`) are measured in bytes, but the SKUs usually use GB
as a unit. When estimating usage, to do this unit conversion, you'd have to multiply or divide by a
constant value to convert to GB. Such constants can be defined in this section.

```YAML
defined_metrics:
    convert_to_gb: 1 / 1000 * 1000 * 1000
```

You can also define computation of the metrics or previously defined variables.

```YAML
defined_values:
    convert_to_gb: 1 / 1000 * 1000 * 1000
    bfe_request_bytes_gb: bfe:request.size_bytes * convert_to_gb
```

The definitions are executed sequentially top to bottom. The values of each of these variables are
computed once, and stored for future use. Redefining the variable will overwrite the earlier value
and future uses will use the new value.

#### Usage Estimation

This section defines the formulas to be applied to calculate the per-SKU usage. Estimates are
grouped in different sections. Summary is produced per section by summing up the usage estimations
(and cost if the sku file is provided)

```YAML
usage_estimations:
  'section-1':
    'sku-1-1': <formula for sku>
    'sku-1-2': <formula for sku>

  'section-2':
    'sku-2-1': <formula for sku>
    'sku-2-2': <formula for sku>
  ...
```

For computing the usage estimations, the given formulas are evaluated with a context of all the
previously defined metrics and variables from the `defined_values` section. Once usage is estimated,
usage per-million is estimated by scaling this usage to a million requests. The value of the
`num_requests_metric` defined in the cost metadata section is used for this purpose.

Sample section:

```YAML
usage_estimations:
  'Buyer-Compute':
    'N2D AMD Instance Core running in Virginia': bfeCoreHours + biddingCoreHours
    'N2D AMD Instance Ram running in Virginia': (bfeCoreHours + biddingCoreHours) * machineTypeRamMultiplier

  'Buyer-Confidential-Compute':
    'Confidential Computing Instance Core': bfeCoreHours + biddingCoreHours
    'Confidential Computing Instance RAM': (bfeCoreHours + biddingCoreHours) * machineTypeRamMultiplier
```

Taking the example of the SKU 'N2D AMD Instance Core running in Virginia', the usage for this is
estimated per hour as `bfeCoreHours + biddingCoreHours`. The variables `bfeCoreHours` and
`biddingCoreHours` were presumably previously defined in the model to calculate the total vCPU hours
the bfe and bidding servers were used (since billing for this compute resource would be per vCPU
hour).

#### Results template

A cost model can optionally define a template for the results. The template can use the following
variables:

-   `$Summary`: This is the summary of all sections
-   `$<section>`: Variables for each section from usage_estimations are defined, and you can use
    them in the template. E.g. For the usage_estimations above, you can use `$Buyer-Compute` and
    `$Buyer-Confidential-Compute` as variables in the template.
-   `$Metrics` - All of the metrics that were used for the computations

A sample template:

```YAML
results_template: '
  $Summary
  $Buyer-Compute
  $Buyer-Confidential-Compute
  $Buyer-Network
  $Metrics'
```

Each of the variables will get replaced with the actual csv that were generated.

If no template is defined, then the following structure is used:

```sh
Summary

Data for all the sections defined in the usage_estimations

All metrics that were used
```

# Debugging

## Checking for errors

If the cost model is not working, check the output of the tool. If variables in the formulas are not
found, an error message will be printed.

## Check the output produced

The output produced has all the metrics that were used for the calculation. These are a good place
to start for debugging.

Sample output:

```csv
Metric, Value
cost_model, Redacted
post.num.bfe, Redacted
post.num.sfe, Redacted
post.num.bidding, Redacted
post.num.auction, Redacted
start_time, Redacted
end_time, Redacted
test.duration, Redacted
sfe:request.size_bytes, Redacted
auction:request.size_bytes, Redacted
bfe:request.size_bytes, Redacted
bidding:request.size_bytes, Redacted
sfe:response.size_bytes, Redacted
```

## Turn on INFO logging

Extra information is printed at the INFO logging level. For using this, add the following command
line option:

```sh
  --loglevel info
```

This will print additional debugging information.

-   **Saved metrics:** Downloaded metrics are always saved in a file, whether or not you asked them
    to be saved. If you did not ask them to be saved in a file, they will be saved in a temp file.
    The name of the file is printed in a log message at INFO level. You can look at this file to see
    the downloaded metrics values and debug further.
-   **Saved Output:** Similar to above, the produced output is also always saved to a file, whether
    or not you asked it to be saved. If no `--output_file` option was used, then the output is saved
    in a temp file. The name of this file will be printed in a log message at the INFO level.
-   **Sqlite3 database:** All internal calculations are performed using an in memory sqlite3
    database. This is backed by a file. We do not delete this file on exit, and the name of this
    file is printed in a log message at the INFO level.

## Using the sqlite3 database for debugging

As mentioned above, internal calculations are performed in a sqlite3 database. This is backed by a
file, the name of which will be printed in a log message at INFO level.

```sh
INFO:tools.cost_estimation.sql_expr:Sqlite db name: /tmp/tmpfbe827hj
```

Once you have the filename of the file backing the internal in-memory Sqlite3 database, you can open
it with the sqlite3 command line, examine the data and see the details of the calculations
performed.

```sql
$ sqlite3 /tmp/tmpfbe827hj
SQLite version 3.45.3 2024-04-15 13:34:05
Enter ".help" for usage hints.
sqlite> .schema SkuEstimates
CREATE TABLE SkuEstimates(description STRING, estimate FLOAT, category STRING);
sqlite> select * from SkuEstimates ;
description                                                   estimate       category
------------------------------------------------------------  -------------  ---------------------------
N2D AMD Instance Core running in Virginia                     Redacted       Seller-Compute

N2D AMD Instance Ram running in Virginia                      Redacted       Seller-Compute

E2 Instance Core running in Virginia                          Redacted       Seller-Compute

E2 Instance Ram running in Virginia                           Redacted       Seller-Compute

Confidential Computing Instance Core                          Redacted       Seller-Confidential-Compute

Confidential Computing Instance RAM                           Redacted       Seller-Confidential-Compute

```
